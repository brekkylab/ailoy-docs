import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeTabs from "@site/src/components/CodeTabs";

# RAG with Vector Store

**Retrieval-Augmented Generation (RAG)** is a powerful method that enhances language models with external knowledge. Instead of relying solely on the modelâ€™s internal training data, RAG retrieves relevant documents from a knowledge baseâ€”typically a vector storeâ€”and injects them into the prompt at inference time. This allows the model to generate more accurate, up-to-date, and context-aware responses.

RAG is particularly useful when:

* The knowledge base is too large to fit within the modelâ€™s context window.
* Real-time or domain-specific information is needed.
* You want to combine static model knowledge with dynamic data sources.

### Initializing a Vector Store

Ailoy simplifies the construction of RAG pipelines through its built-in `VectorStore` component, which works alongside the `Agent`.

To initialize a vector store:

<CodeTabs>
<CodeTabs.Python>
```python
from ailoy import Runtime
from ailoy.vectorstore import VectorStore, FAISSConfig

rt = Runtime()
with VectorStore(rt, FAISSConfig(embedding="bge-m3")) as vs:
    ...
```
</CodeTabs.Python>
<CodeTabs.Nodejs>
```typescript
import { createRuntime, VectorStore } from "ailoy-node";

const rt = await createRuntime();
const vs = new VectorStore(rt, {
  type: "faiss",
  embedding: "bge-m3",
});
await vs.initialize();
```
</CodeTabs.Nodejs>
</CodeTabs>

> Ailoy currently supports both [**FAISS**](https://github.com/facebookresearch/faiss) and [**ChromaDB**](https://www.trychroma.com/) as vector store backends.
> Refer to the official configuration guide for backend-specific options.

> ðŸ’¡ **Note:** At this time, the only supported embedding model is [`bge-m3`](https://huggingface.co/BAAI/bge-m3).
> Additional embedding models will be supported in future releases.

### Inserting Documents into the Vector Store

You can insert text along with optional metadata into the vector store:

<CodeTabs>
<CodeTabs.Python>
```python
vs.insert(
    "Ailoy is a lightweight library for building AI applications", 
    metadata={"topic": "Ailoy"}
)
```
</CodeTabs.Python>
<CodeTabs.Nodejs>
```typescript
await vs.insert({
  document: "Ailoy is a lightweight library for building AI applications",
  metadata: {
    topic: "Ailoy",
  },
});
```
</CodeTabs.Nodejs>
</CodeTabs>

In practice, you should split large documents into smaller chunks before inserting them. This improves retrieval quality. You may use any text-splitting tool (e.g., [LangChain](https://python.langchain.com/docs/concepts/text_splitters/)), or utilize Ailoyâ€™s low-level runtime API for text splitting. (See [Calling Low-Level APIs](./calling-low-level-apis.md) for more details.)

### Retrieving Relevant Documents

To retrieve documents similar to a given query:

<CodeTabs>
<CodeTabs.Python>
```python
query = "What is Ailoy?"
items = vs.retrieve(query, top_k=5)
```
</CodeTabs.Python>
<CodeTabs.Nodejs>
```typescript
const query = "What is Ailoy?";
const items = await vs.retrieve(query, 5);
```
</CodeTabs.Nodejs>
</CodeTabs>

This returns a list of `VectorStoreRetrieveItem` instances representing the most relevant chunks, ranked by similarity. The number of results is controlled via the `top_k` parameter (default is 5).

### Constructing an Augmented Prompt

Once documents are retrieved, you can construct a context-enriched prompt as follows:

<CodeTabs>
<CodeTabs.Python>
```python
prompt = f"""
    Based on the provided contexts, try to answer user's question.
    Context: {[item.document for item in items]}
    Question: {query}
"""
```
</CodeTabs.Python>
<CodeTabs.Nodejs>
```typescript
const prompt = `
  Based on the provided contexts, try to answer user' question.
  Context: ${items.map((item) => item.document)}
  Question: ${query}
`;
```
</CodeTabs.Nodejs>
</CodeTabs>

You can then pass this prompt to the agent for inference:

<CodeTabs>
<CodeTabs.Python>
```python
for resp in agent.run(prompt):
    print(resp.content, end='')
print()
```
</CodeTabs.Python>
<CodeTabs.Nodejs>
```typescript
for await (const resp of agent.run(prompt)) {
    process.stdout.write(resp.content);
}
process.stdout.write("\n");
```
</CodeTabs.Nodejs>
</CodeTabs>

### Complete Example

<CodeTabs>
<CodeTabs.Python>
```python
from ailoy import Runtime
from ailoy.vectorstore import VectorStore, FAISSConfig

# Initialize Runtime
rt = Runtime()
# Initialize Agent and VectorStore
with Agent(rt, model_name="qwen3-8b") as agent, VectorStore(rt, FAISSConfig(embedding="bge-m3")) as vs:
    # Insert items
    vs.insert(
        "Ailoy is a lightweight library for building AI applications", 
        metadata={"topic": "Ailoy"}
    )

    # Search the most relevant items
    query = "What is Ailoy?"
    items = vs.retrieve(query)

    # Augment user query
    prompt = f"""
        Based on the provided contexts, try to answer user's question.
        Context: {[item.document for item in items]}
        Question: {query}
    """

    # Invoke agent
    for resp in agent.run(prompt):
        print(resp.content, end='')
    print()
```
</CodeTabs.Python>
<CodeTabs.Nodejs>
```typescript
import { createRuntime, createAgent, createVectorStore } from "ailoy-node";

async function main() {
  // Initialize Runtime
  const rt = await createRuntime();
  // Initialize Agent
  const agent = await createAgent(rt, {
    model: {
      name: "qwen3-8b",
    },
  });
  // Initialize VectorStore
  const vs = await createVectorStore(rt, {
    type: "faiss",
    embedding: "bge-m3",
  });

  // Insert items
  await vs.insert({
    document: "Ailoy is a lightweight library for building AI applications",
    metadata: {
        topic: "Ailoy",
    },
  });

  // Search the most relevant items
  const query = "What is Ailoy?";
  const items = await vs.retrieve(query, 5);

  // Augment user query
  const prompt = `
    Based on the provided contexts, try to answer user' question.
    Context: ${items.map((item) => item.document)}
    Question: ${query}
  `;

  // Invoke agent
  for await (const resp of agent.run(prompt)) {
    process.stdout.write(resp.content);
  }
  process.stdout.write("\n");
}
```
</CodeTabs.Nodejs>
</CodeTabs>

> **Tip:** For best results, ensure your documents are chunked semantically (e.g., by paragraphs or sections).
